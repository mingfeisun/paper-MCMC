\begin{abstract}
We consider distribution matching approaches for imitation learning and propose a unified formulation for different statistical distance measures.  
imitation learning and show its link to actor-critic algorithms.
Existing studies attempt to unify different imitation learning methods into one framework by interpreting them as results of using different statistical divergence to measure the distribution discrepancy. 
These attempts, however, have three key limitations:
lack of consider other statistical metrics than only divergence measures, 
few or even contradicting clues on what the statistical distance should be used;
and lack of insights of how the changing discriminator could introduce bias and variance in the learning process. 
To overcome these limitations, we introduce a unified formulation and show how previous methods can fit into the proposed formulation. 
By this unified formulation, we analyze the links between the distribution matching for imitation learning and reinforcement learning methods, particularly policy gradient methods.
Our analysis outlines the selection 
of distribution distances and also yields a novel and more sample-efficient imitation algorithm, Deterministic Adversarial Imitation Learning (DAIL). 
Empirical results show that DAIL outperforms previous methods across all environments and by a large margin. 
\end{abstract}