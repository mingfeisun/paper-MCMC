\begin{abstract}
We consider a specific problem of imitation learning that learns a policy which generates similar behaviors as demonstrations, rather than optimal behaviors under some unknown rewards. 
One approach to solve this problem is to match the state-action distribution induced by the policy to the state-action distribution showed by the demonstration. 
We point out that such approach has two limitations: 
no consensus or guidelines on choosing the distance for distribution comparison, and lack of uniform formulation of the distribution matching for imitation learning.
To overcome these limitations, we introduce a uniform formulation and show how previous methods can fit into the proposed formulation. 
By this uniform formulation, we analyze the links between the distribution matching for imitation learning and reinforcement learning methods, particularly policy gradient methods.
Our analysis outlines the selection of distribution distances and also yields a novel and more sample-efficient imitation algorithm, Deterministic Adversarial Imitation Learning (DAIL). 
Empirical results show that DAIL outperforms previous methods across all environments and by a large margin. 
\end{abstract}