\begin{abstract}
We consider a specific problem of imitation learning that learns a policy which generates similar behaviors as demonstrations, rather than optimal behaviors under some unknown rewards. 
One approach to solve this problem is to match the state-action distribution induced by the policy to the state-action distribution showed by the demonstration. 
Such an approach has two limitations: 
lack of consensus on how to choose the distance for distribution comparison, and lack of uniform formulation \sw{what is a uniform formulation?} for imitation learning.
To overcome these limitations, we introduce a uniform formulation and show how previous methods can fit into the proposed formulation. 
By this uniform formulation, we analyze the links between the distribution matching for imitation learning and reinforcement learning methods, particularly policy gradient methods.
Our analysis outlines the selection \sw{what does it mean to outline a selection?} of distribution distances and also yields a novel and more sample-efficient imitation algorithm, Deterministic Adversarial Imitation Learning (DAIL). 
Empirical results show that DAIL outperforms previous methods across all environments and by a large margin. 
\end{abstract}