\section{Distribution matching formulation}

\subsection{Background}
An infinite-horizon, discounted Markov Decision Process (MDP) is modeled by tuple $(\mathcal{S} , \mathcal{A}, P, r , \rho_0, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ denotes the state transition probability, $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ represents the reward function, $\rho_0:\mathcal{S} \rightarrow\mathcal{A}$ is the initial state distribution, and $\gamma\in(0, 1]$ is a discount factor. 
A stochastic policy $\pi\in\Pi$ is $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$. Let $\tau_E$ denote a trajectory sampled from expert policy $\pi_E$: $\tau_E = \big[ (s_0, a_0), (s_1, a_1), ..., (s_n, a_n) \big]$. 

In the average reward formulation, $d_\pi$ is defined as $d_{\pi}=\lim_{t\leftarrow\infty}Pr(s_t=s, a_t=a|s_0, \pi)$, which we assume exists and is independent of $s_0$ for all policies. 
\begin{equation}\label{equ:average_reward}
d_{\pi}(s^\prime, a^\prime) = \sum_{s, a} \pi(a^\prime|s^\prime) T_{\pi}(s^\prime|s, a) d_{\pi}(s, a), \quad \forall s^\prime, a^\prime
\end{equation}

In the discounted reward formulation, $d_\pi$ is defined as $d_{\pi}=\sum_{t=0}^{\infty}\gamma^t Pr(s_t=s, a_t=a|s_0, \pi)$
% $(s, a)$ distribution
% \begin{equation}
% d(s^\prime, a^\prime) = \underbrace{(1-\gamma) d_{0}(s^\prime, a^\prime) + \gamma \int \pi(a^\prime|s^\prime) P(s^\prime|s, a) d(s, a) ds da}_{(\mathcal{T}\circ d) (s^\prime, a^\prime)}, \quad \forall (s^\prime, a^\prime)\in \mathcal{S}\times\mathcal{A}. 
% \end{equation}
\begin{equation}
\gamma \sum_{s, a}\pi(a^\prime|s^\prime) T_{\pi}(s^\prime |s, a) d_{\pi}(s, a) - d_{\pi}(s^\prime, a^\prime) + (1-\gamma)d_0(s^\prime, a^\prime) = 0, \quad s^\prime, a^\prime
\end{equation}


% \begin{lemma} Denote by $(s, a, s^\prime)\sim d_{\pi}$ draws from $d_{\pi}(s)\pi(a|s)T(s^\prime|s, a)$. Equation~\ref{equ:average_reward} holds if and only if, for any function $f$, we have
% \begin{equation}
% \mathbb{E}_{(s,a)\sim d_{\pi}(s,a)}\big[ f(s,a) \big] - \mathbb{E}_{(s, a, s^\prime,a^\prime) \sim d_{\pi}(s,a)}\big[ f(s^\prime,a^\prime) \big] = 0
% \end{equation}
% \end{lemma}
% 
% \begin{proof}
% $(\rightarrow)$
% \begin{align*}
% &\mathbb{E}_{(s, a)\sim d_{\pi}(s, a)}\big[ f(s, a) \big] - \mathbb{E}_{(s, a, s^\prime, a^\prime) \sim d_{\pi}(s, a)}\big[ f(s^\prime, a^\prime) \big] \\
% = & \sum_{s, a} d_{\pi}(s, a)f(s, a) - \sum_{s,a,s^\prime,a^\prime} d_{\pi}(s, a)T(s^\prime|s, a)\pi(a^\prime|s^\prime) f(s^\prime, a^\prime) \\
% = & \sum_{s^\prime, a^\prime}d_{\pi}(s^\prime, a^\prime)f(s^\prime, a^\prime) - \sum_{s^\prime,a^\prime,s,a} d_{\pi}(s, a)T(s^\prime|s, a)\pi(a^\prime|s^\prime)f(s^\prime, a^\prime) \\
% = & \sum_{s^\prime,a^\prime} f(s^\prime,a^\prime) \big[ d_{\pi}(s^\prime,a^\prime) - \sum_{s,a}T(s^\prime|s,a)\pi(a^\prime|s^\prime) d_{\pi}(s,a) \big] \\
% = & 0
% \end{align*}
% 
% $(\leftarrow)$
% If for any function $f$, we have
% $\mathbb{E}_{s\sim d_{\pi}(s)}\big[ f(s) \big] - \mathbb{E}_{(s, a, s^\prime) \sim d_{\pi}(s)}\big[ f(s^\prime) \big] = 0$, then based on the above analysis, we have $d_{\pi}(s^\prime) - \sum_{s}T(s^\prime|s) d_{\pi}(s) =0 $
% \end{proof}

% Denote $G(s, a, s^\prime, a^\prime) = \pi(a^\prime|s^\prime) T(s^\prime|s, a)$, we have
% \begin{equation}
% \mathbb{E}_{(s,a)\sim d_{\pi}}\big[ f(s,a) \big] - \mathbb{E}_{(s, a, s^\prime, a^\prime) \sim G, (s,a)\sim d_{\pi}}\big[ f(s^\prime, a^\prime) \big] = 0
% \end{equation}

% In order to solve this problem, we can use the min-max optimization, i.e., 
% \begin{equation}
% \min_{G} \max_{f} \mathbb{E}_{(s,a)\sim d_{\pi}}\big[ D(s) \big] - \mathbb{E}_{s^\prime \sim G_a(s, s^\prime), s\sim d_{\pi}}\big[ D(s^\prime) \big]
% \end{equation}
% We can choose both $D$ and $G$ to be neural networks, for which the min-max optimization can be solved numerically by a generative adversarial nets~\citep{goodfellow2014generative}. 


% \begin{lemma}
% \citep{liu2018breaking} Denote by $(s, a, s^\prime)\sim d_{\pi}$ draws from $d_{\pi}(s)\pi(a|s)T(s^\prime|s, a)$. For any function $D$, we have
% \begin{equation}
% \mathbb{E}_{(s,a,s^\prime)\sim d_{\pi}} \big[ \gamma D(s^\prime) - D(s) \big] + (1-\gamma) \mathbb{E}_{s\sim d_0}\big[ D(s) \big] = 0
% \end{equation}
% \end{lemma}
% 
% \begin{proof}
% 
% \end{proof}

% Similarly, we can formulate the min-max optimization as follows
% \begin{align}
% \min_{G}\max_{D} & \mathbb{E}_{s\sim d_{\pi}} \big[ D(s) \big] - \mathbb{E}_{s^\prime\sim G_a(s, s^\prime), s\sim d_{\pi}} \big[ \gamma D(s^\prime) \big] \nonumber\\
% & - (1-\gamma) \mathbb{E}_{s\sim d_0}\big[ D(s) \big]
% \end{align}

% One may view $d_{\pi}$ as the invariant distribution of an induced Markov chain with transition probability of $(1-\gamma) d_{0}(s^\prime) + \gamma T_{\pi}(s^\prime|s)$, which follows $T_{\pi}$ with probability $\gamma$, and restarts from initial distribution $d_{0}(s^\prime)$ with probability $1-\gamma$. 

% \begin{equation*}
% \mathbf{d}_{\pi} = (1-\gamma) \mathbf{d}_0 + \gamma \mathbf{P}_{\pi}^T \mathbf{d}_{\pi}.
% \end{equation*}
% 
% \begin{equation*}
% \mathbf{d}_{\pi} = (1-\gamma) (\mathbf{I} - \gamma \mathbf{P}_{\pi}^T)^{-1}\mathbf{d}_0.
% \end{equation*}

\subsection{Empirical forms of distribution distances}

\paragraph{Distribution distances}
We introduce several common distribution distances. Specifically, to measure the discrepancy between two distributions, we can use \textit{Kullback-Leibler} (KL) divergence, \textit{Jensen-Shannon} (JS) divergence, and \textit{Wasserstein-1} metric. Given two distributions $\mathbb{P}_\pi$ and $\mathbb{P}_{\pi_*}$, their analytical forms are given as below:
\begin{itemize}
    \item KL divergence: 
    $\text{KL}(\mathbb{P}_\pi || \mathbb{P}_{\pi_*}) = \int \log\Big( \frac{P_\pi(x)}{P_{\pi_*}(x)} \Big) P_{\pi}(x) dx$.
    
    \item JS divergence:
    $\text{JS}(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \text{KL}(\mathbb{P}_\pi || \mathbb{P}_{\pi_*}) + \text{KL}(\mathbb{P}_{\pi_*} || \mathbb{P}_{\pi})$.
    
    \item Wasserstein-1 metric:
    $W(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \inf_{\gamma\in\Pi(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*})} \mathbb{E}_{(x, y)\sim\gamma}\big[ ||x-y|| \big]$, 
    where $\Pi(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*})$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are respectively $\mathbb{P}_{\pi}$ and $\mathbb{P}_{\pi_*}$ .
\end{itemize}

\paragraph{Empirical forms} We now show how these distribution distances can be translated into another forms, which we call \textit{empirical forms} as they can estimated via samples. KL divergence and JS divergence are actually two variants of $f$-divergence. Based on the variational lower bounds of $f$-divergence\citep{nguyen2010estimating}, we have
\begin{equation*}
D_{f} (\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \sup_{\phi\in \Phi} \mathbb{E}_{x\sim\mathbb{P}_\pi}[-f^*(\phi) ] + \mathbb{E}_{x\sim\mathbb{P}_{\pi_*}}[\phi(x)],
\end{equation*}
where $f^*$ is the \textit{convex conjugate} for $f$, defined as $f^*(v) := \sup_{u\in\mathbb{R}} \{ u \cdot v - f(u) \}$. Similarly for Wasserstein-1 metric, we also have its lower bound~\citep{villani2008optimal}:
\begin{equation*}
W(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \sup_{||f||_{L}\leq 1} \mathbb{E}_{x\sim\mathbb{P}_\pi}[f(x)] + \mathbb{E}_{x\sim\mathbb{P}_{\pi_*}}[-f(x)].
\end{equation*}

\paragraph{Problem formulation} We generalize the idea of using empirical forms to estimate the analytical correspondence, and assume that: for any distribution distances used in the distribution matching, we can always have the empirical forms, and can convert the analytical form to empirical as follows:
\begin{equation*}
\min_{\pi} \underbrace{D\big(d_{\pi}(s, a), d_{\pi_*}(s, a)\big)}_{\text{Analytical form}} \quad \Rightarrow \quad \min_{\pi} \sup_{f\in\mathcal{F}} \underbrace{ \mathbb{E}_{(s, a)\sim d_{\pi}} \big[ f(s, a) \big] + \mathbb{E}_{(s, a)\sim d_*} \big[\tilde{f}(s, a) \big]}_{\text{Empirical form}},
\end{equation*}
where $f(s, a)$ is a function specific to the distribution measure $D(\cdot||\cdot)$ and $\tilde{f}(s, a)$ is defined according to $f(s, a)$. Now we have the \textbf{general objective function} for the distribution matching in imitation learning:
\begin{equation}\label{equ:general_objective}
\min_{\pi} \sup_{f\in\mathcal{F}} \mathbb{E}_{(s, a)\sim d_{\pi}} \big[ f(s, a) \big] + \mathbb{E}_{(s, a)\sim d_*} \big[\tilde{f}(s, a) \big].
\end{equation}
This generalization implies that, regardless of the distribution distances we choose, ultimately we need to optimize an objective which shares the same structure as Equation~\ref{equ:general_objective}. 

% The objective of imitation learning is to find a policy $\pi$ such that its performance is close to the expert's performance. 
% \begin{align*}
% & \min_{\pi} \big| \eta(\pi_E) - \eta(\pi) \big| \\ 
% & \Rightarrow \min_{\pi} \Big| \mathbb{E}_{(s, a)\sim d_{E}} [r(s, a)] - \mathbb{E}_{(s, a)\sim d_{\pi}} [r(s, a)] \Big|. 
% \end{align*} 
% Since the reward function $r(s, a)$ is unknown to us in imitation learning, directly optimizing the above objective is intractable. We consider a more strict formulation:
% \begin{equation*}
% \min_{\pi} \sup_{r\in\mathcal{R}} \Big| \mathbb{E}_{(s, a)\sim d_{E}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a) \big] \Big|. 
% \end{equation*} 
% It is easily verified that if for every $r\in \mathcal{R}$ we have $-r \in \mathcal{R}$, then $\sup_{r\in\mathcal{R}} \big[ \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi_E}} \big[r(s, a) \big] \big]$ is non-negative, satisfies the triangular inequality, and is symmetric.
% We can then simply remove the absolute operator and formulate the problem as follows:
% \begin{equation*}
% \min_{\pi} \sup_{r\in\mathcal{R}} \Big[ \mathbb{E}_{(s, a)\sim d_{E}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a) \big] \Big]. 
% \end{equation*} 


% Based on Theorem~\ref{theo:policy_gradient} and actor-critic algorithm, we can actually substitute $\Psi(s_t, a_t)$ with $\hat{r}(s_t, a_t) + V(s_{t+1}) - V(s_t)$, where $V(s)$ is defined as $V(s) = \sum_{a}\pi(s, a)\Psi(s, a)$. 


% \begin{equation*}
% \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}[\phi^*(f(s, a))] =  \mathbb{E}_{(s, a)\sim d_{*}}\big[ \omega(s, a) \cdot \phi^*(f(s, a)) \big]
% \end{equation*}
% \begin{equation*}
% \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}[\nabla_\theta\log \pi_{\theta}(s, a) \cdot \Psi(s, a)] =  \mathbb{E}_{(s, a)\sim d_{*}}\big[ \omega(s, a) \cdot \nabla_\theta\log \pi_{\theta}(s, a) \cdot \Psi(s, a) \big]
% \end{equation*}
% where $\omega(s, a)$ is defined as $\omega(s, a) = \frac{d_{\pi_{\theta}}(s, a)}{d_*(s, a)}$, also called \textit{density ratio} in off-policy policy evaluation~\citep{}. 


% Problem formulation:
% \begin{align}
% \min_{\pi} \quad & D_{f} \big(d_*(s, a) || d_{\pi}(s, a) \big) \\
% \text{subject to} \quad & d_{\pi}(s, a) = (\mathcal{T} \circ d_{\pi})(s, a) \quad \forall s\in \mathcal{S}, a\in \mathcal{A}. \nonumber
% \end{align}


% Substitute the objective and plug in $x$ as $(s, a)$:
% \begin{align*}
% \min_{\pi} \sup_{\phi\in\Phi} \quad & \mathbb{E}_{(s, a)\sim d_\pi} \big[ \phi(s, a) \big] - \mathbb{E}_{(s, a)\sim d_*} \big[ f^*(\phi(s, a)) \big], \\
% \text{subject to} \quad & d_{\pi}(s, a) = (\mathcal{T} \circ d_{\pi})(s, a) \quad \forall s\in \mathcal{S}, a\in \mathcal{A}. \nonumber
% \end{align*}





% $(s, a)$ distribution
% \begin{equation}
% d(s^\prime, a^\prime) = \underbrace{(1-\gamma) d_{0}(s^\prime, a^\prime) + \gamma \int \pi(a^\prime|s^\prime) P(s^\prime|s, a) d(s, a) ds da}_{(\mathcal{T}\circ d) (s^\prime, a^\prime)}, \quad \forall (s^\prime, a^\prime)\in \mathcal{S}\times\mathcal{A}. 
% \end{equation}
