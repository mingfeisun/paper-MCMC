\section{Distribution matching formulation}

\subsection{Background}
An finite-horizon, discounted Markov Decision Process (MDP) is modeled by tuple $(\mathcal{S} , \mathcal{A}, P, r , d_0, \gamma, T)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ is the action space, $P:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow \mathbb{R}$ denotes the state transition probability, $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$ represents the reward function, $d_0:\mathcal{S} \rightarrow\mathbb{R}$ is the initial state distribution, $\gamma\in[0, 1)$ is a discount factor, and $T\in \mathbb{N}^+$ is the time horizon.
A stochastic policy $\pi\in\Pi$ is $\pi: \mathcal{S}\times\mathcal{A}\rightarrow [0, 1]$.
Let a trajectory be a sequence of state-action pairs $\tau=\{s_0, a_0, s_1, a_1, ..., a_{T-1}, s_T\}$
A stochastic policy $\pi$ induces a  distribution of trajectories $d_{\pi}(\tau)$ defined as follows:
\begin{equation}\label{equ:d_pi_definition}
d_{\pi}(\tau) = d_0(s_0)\prod_{t=1}^{T}\pi(s_{t-1}, a_{t-1})P(s_{t}|s_{t-1}, a_{t-1})
\end{equation}
We slightly abuse notation by also denoting the state distribution as $d_\pi(s)$: $d_{\pi}(s) =\lim_{t\rightarrow\infty}Pr(s_t=s, a_t=a|d_0, \pi)$, 
which we assume exists and is independent of the initial state distribution $d_0$ for all policies. 
For state-action distribution $d_{\pi}(s, a)$, we have $d_{\pi}(s, a) = d_{\pi}(s)\pi(a|s)$. 
We also denote the distribution of demonstrations as $d_{*}(\tau)$ and assume it is given in the form of trajectory samples. 
% We define the distribution of a stochastic policy $\pi$.  

% \vit{Should this be a distribution over actions?} 
% In the average reward formulation, the state distribution $d_\pi(s)$ is defined as $d_{\pi}(s) =\lim_{t\rightarrow\infty}Pr(s_t=s, a_t=a|d_0, \pi)$, which we assume exists and is independent of the initial state distribution $d_0$ for all policies. 
% Based on the stationarity of a Markov chain, we have the following for stationary state-action distributions:
% \begin{equation*}\label{equ:average_reward}
% d_{\pi}(s^\prime, a^\prime) = \sum_{s, a} \pi(a^\prime|s^\prime) T_{\pi}(s^\prime|s, a) d_{\pi}(s, a), \quad \forall s^\prime, a^\prime.
% \end{equation*}
% In the discounted reward formulation, $d_\pi(s)$ is defined as $d_{\pi}(s)=\sum_{t=0}^{\infty}\gamma^t Pr(s_t=s, a_t=a|d_0, \pi)$. Similarly, we have the following for stationary state-action distributions: 
% $(s, a)$ distribution
% \begin{equation}
% d(s^\prime, a^\prime) = \underbrace{(1-\gamma) d_{0}(s^\prime, a^\prime) + \gamma \int \pi(a^\prime|s^\prime) P(s^\prime|s, a) d(s, a) ds da}_{(\mathcal{T}\circ d) (s^\prime, a^\prime)}, \quad \forall (s^\prime, a^\prime)\in \mathcal{S}\times\mathcal{A}. 
% \end{equation}
% \begin{equation*}
% d_{\pi}(s^\prime, a^\prime) = \gamma \sum_{s, a}\pi(a^\prime|s^\prime) T_{\pi}(s^\prime |s, a) d_{\pi}(s, a) + (1-\gamma)d_0(s^\prime, a^\prime), \quad \forall s^\prime, a^\prime. 
% \end{equation*}
% Intuitively, this stationarity equation simply reflects the conservation of flow of the stationary distribution: at each timestep, the flow out of $(s, a)$ (the LHS) must equal the flow into $(s^\prime, a^\prime)$ (the RHS). And in the discounted state-action distribution, such flow conservation should also take into account the initial state-action distribution, i.e., $d_0(s^\prime, a^\prime)$. 

% \vit{$\forall s',a'$? Give some intuition about eq 2.}


% \begin{lemma} Denote by $(s, a, s^\prime)\sim d_{\pi}$ draws from $d_{\pi}(s)\pi(a|s)T(s^\prime|s, a)$. Equation~\ref{equ:average_reward} holds if and only if, for any function $f$, we have
% \begin{equation}
% \mathbb{E}_{(s,a)\sim d_{\pi}(s,a)}\big[ f(s,a) \big] - \mathbb{E}_{(s, a, s^\prime,a^\prime) \sim d_{\pi}(s,a)}\big[ f(s^\prime,a^\prime) \big] = 0
% \end{equation}
% \end{lemma}
% 
% \begin{proof}
% $(\rightarrow)$
% \begin{align*}
% &\mathbb{E}_{(s, a)\sim d_{\pi}(s, a)}\big[ f(s, a) \big] - \mathbb{E}_{(s, a, s^\prime, a^\prime) \sim d_{\pi}(s, a)}\big[ f(s^\prime, a^\prime) \big] \\
% = & \sum_{s, a} d_{\pi}(s, a)f(s, a) - \sum_{s,a,s^\prime,a^\prime} d_{\pi}(s, a)T(s^\prime|s, a)\pi(a^\prime|s^\prime) f(s^\prime, a^\prime) \\
% = & \sum_{s^\prime, a^\prime}d_{\pi}(s^\prime, a^\prime)f(s^\prime, a^\prime) - \sum_{s^\prime,a^\prime,s,a} d_{\pi}(s, a)T(s^\prime|s, a)\pi(a^\prime|s^\prime)f(s^\prime, a^\prime) \\
% = & \sum_{s^\prime,a^\prime} f(s^\prime,a^\prime) \big[ d_{\pi}(s^\prime,a^\prime) - \sum_{s,a}T(s^\prime|s,a)\pi(a^\prime|s^\prime) d_{\pi}(s,a) \big] \\
% = & 0
% \end{align*}
% 
% $(\leftarrow)$
% If for any function $f$, we have
% $\mathbb{E}_{s\sim d_{\pi}(s)}\big[ f(s) \big] - \mathbb{E}_{(s, a, s^\prime) \sim d_{\pi}(s)}\big[ f(s^\prime) \big] = 0$, then based on the above analysis, we have $d_{\pi}(s^\prime) - \sum_{s}T(s^\prime|s) d_{\pi}(s) =0 $
% \end{proof}

% Denote $G(s, a, s^\prime, a^\prime) = \pi(a^\prime|s^\prime) T(s^\prime|s, a)$, we have
% \begin{equation}
% \mathbb{E}_{(s,a)\sim d_{\pi}}\big[ f(s,a) \big] - \mathbb{E}_{(s, a, s^\prime, a^\prime) \sim G, (s,a)\sim d_{\pi}}\big[ f(s^\prime, a^\prime) \big] = 0
% \end{equation}

% In order to solve this problem, we can use the min-max optimization, i.e., 
% \begin{equation}
% \min_{G} \max_{f} \mathbb{E}_{(s,a)\sim d_{\pi}}\big[ D(s) \big] - \mathbb{E}_{s^\prime \sim G_a(s, s^\prime), s\sim d_{\pi}}\big[ D(s^\prime) \big]
% \end{equation}
% We can choose both $D$ and $G$ to be neural networks, for which the min-max optimization can be solved numerically by a generative adversarial nets~\citep{goodfellow2014generative}. 


% \begin{lemma}
% \citep{liu2018breaking} Denote by $(s, a, s^\prime)\sim d_{\pi}$ draws from $d_{\pi}(s)\pi(a|s)T(s^\prime|s, a)$. For any function $D$, we have
% \begin{equation}
% \mathbb{E}_{(s,a,s^\prime)\sim d_{\pi}} \big[ \gamma D(s^\prime) - D(s) \big] + (1-\gamma) \mathbb{E}_{s\sim d_0}\big[ D(s) \big] = 0
% \end{equation}
% \end{lemma}
% 
% \begin{proof}
% 
% \end{proof}

% Similarly, we can formulate the min-max optimization as follows
% \begin{align}
% \min_{G}\max_{D} & \mathbb{E}_{s\sim d_{\pi}} \big[ D(s) \big] - \mathbb{E}_{s^\prime\sim G_a(s, s^\prime), s\sim d_{\pi}} \big[ \gamma D(s^\prime) \big] \nonumber\\
% & - (1-\gamma) \mathbb{E}_{s\sim d_0}\big[ D(s) \big]
% \end{align}

% One may view $d_{\pi}$ as the invariant distribution of an induced Markov chain with transition probability of $(1-\gamma) d_{0}(s^\prime) + \gamma T_{\pi}(s^\prime|s)$, which follows $T_{\pi}$ with probability $\gamma$, and restarts from initial distribution $d_{0}(s^\prime)$ with probability $1-\gamma$. 

% \begin{equation*}
% \mathbf{d}_{\pi} = (1-\gamma) \mathbf{d}_0 + \gamma \mathbf{P}_{\pi}^T \mathbf{d}_{\pi}.
% \end{equation*}
% 
% \begin{equation*}
% \mathbf{d}_{\pi} = (1-\gamma) (\mathbf{I} - \gamma \mathbf{P}_{\pi}^T)^{-1}\mathbf{d}_0.
% \end{equation*}

\subsection{Empirical forms of statistical distances}

\paragraph{Statistical distances} 
% \sw{Is it accurate to call these distances given eg the KL is not a metric?}
We introduce several common statistical distances. Specifically, to measure the discrepancy between two distributions, we can use the $f$-divergence (which includes \textit{Kullback-Leibler} (KL) divergence, Hellinger distance, and total variation distance), and \textit{Jensen-Shannon} (JS) divergence, and \textit{Wasserstein-1} metric. Given two distributions $\mathbb{P}_\pi$ and $\mathbb{P}_{\pi_*}$, their analytical forms are given as below:
\begin{itemize}
    \item KL divergence: 
    $\text{KL}(\mathbb{P}_\pi || \mathbb{P}_{\pi_*}) = \int \log\Big( \frac{P_\pi(x)}{P_{\pi_*}(x)} \Big) P_{\pi}(x) dx$.
    
    \item JS divergence:
    $\text{JS}(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \text{KL}(\mathbb{P}_\pi || \mathbb{M}) + \text{KL}(\mathbb{P}_{\pi_*} || \mathbb{M})$.  where $\mathbb{M} = \frac{1}{2}(\mathbb{P}_{\pi_*} +\mathbb{P}_{\pi})$ 
    
    \item Wasserstein-1 metric:
    $W(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \inf_{\gamma\in\Pi(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*})} \mathbb{E}_{(x, y)\sim\gamma}\big[ ||x-y|| \big]$, 
    where $\Pi(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*})$ denotes the set of all joint distributions $\gamma(x, y)$ whose marginals are respectively $\mathbb{P}_{\pi}$ and $\mathbb{P}_{\pi_*}$ .
\end{itemize}
% \vit{Not sure how rigorous this should be, sometimes people mention, that in KL the denominator can be zero only when nominator is zero as well.}

\paragraph{Empirical forms} We now show how these distribution distances can be translated into another forms, which we call \textit{empirical forms} as they can estimated via samples. KL divergence and JS divergence are actually two variants of $f$-divergence. Based on the variational lower bounds of the $f$-divergence \citep{nguyen2010estimating}, we have
\begin{equation*}
D_{f} (\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \sup_{\phi\in \Phi} \mathbb{E}_{x\sim\mathbb{P}_\pi}\big[-f^*\big(\phi(x)\big) \big] + \mathbb{E}_{x\sim\mathbb{P}_{\pi_*}}[\phi(x)],
\end{equation*}
where $f^*$ is the \textit{convex conjugate} for $f$, defined as $f^*(v) := \sup_{u\in\mathbb{R}} \{ uv - f(u) \}$. Similarly for Wasserstein-1 metric, we also have its lower bound~\citep{villani2008optimal}:
\begin{equation*}
W(\mathbb{P}_{\pi}, \mathbb{P}_{\pi_*}) = \sup_{||f||_{L}\leq 1} \mathbb{E}_{x\sim\mathbb{P}_\pi}[f(x)] + \mathbb{E}_{x\sim\mathbb{P}_{\pi_*}}[-f(x)], 
\end{equation*}
where $||f||_{L}\leq 1$ denotes the set of 1-Lipschitz functions.  

% \vit{Did not check the two equations for D and W.}
% \vit{This might be useful: \url{https://arxiv.org/abs/1606.00709}}

\paragraph{Problem formulation} We generalize the idea of using empirical forms to estimate the analytical correspondence, and assume that, for any distribution distances used in the distribution matching, we can always convert the analytical form to empirical as follows:
\begin{equation*}
\min_{\pi} \underbrace{D\big(d_{\pi}(\tau), d_{\pi_*}(\tau)\big)}_{\text{Analytical form}} \quad \Rightarrow \quad \min_{\pi} \sup_{f\in\mathcal{F}} \underbrace{ \mathbb{E}_{\tau\sim d_{\pi}(\tau)} \big[ f(\tau) \big] + \mathbb{E}_{\tau\sim d_*(\tau)} \big[\tilde{f}(\tau) \big]}_{\text{Empirical form}},
\end{equation*}
\sw{Why the min over $\pi$ here if we are just defining divergences, not objectives?}
where $f(\tau)$ is a function specific to the distribution measure $D(\cdot||\cdot)$ and $\tilde{f}(\tau)$ is defined according to $f(s, a)$. \sw{vague: defined how?} \red{given examples} 
Note that we use trajectories $\tau$ instead of the state-action pairs $(s, a)$ as in imitation learning the trajectories are identically independent distributed (i.i.d) while the state-action pairs are not. 
This formulation is also different from what is adopted by 
Now we have the \textbf{unified objective function} for the distribution matching in imitation learning:
\begin{equation}\label{equ:general_objective}
\min_{\pi} \sup_{f\in\mathcal{F}} \mathbb{E}_{\tau\sim d_{\pi}(\tau)} \big[ f(\tau) \big] + \mathbb{E}_{\tau\sim d_*(\tau)} \big[\tilde{f}(\tau) \big].
\end{equation}
\sw{Isn't this exactly the same as the RHS above?}
% \vit{What is $\tilde{f}$?}
This generalization implies that, regardless of the distribution distances we choose, ultimately we need to optimize an objective which shares the same structure as Equation~\ref{equ:general_objective}. \red{entropy regulizer}

% The objective of imitation learning is to find a policy $\pi$ such that its performance is close to the expert's performance. 
% \begin{align*}
% & \min_{\pi} \big| \eta(\pi_E) - \eta(\pi) \big| \\ 
% & \Rightarrow \min_{\pi} \Big| \mathbb{E}_{(s, a)\sim d_{E}} [r(s, a)] - \mathbb{E}_{(s, a)\sim d_{\pi}} [r(s, a)] \Big|. 
% \end{align*} 
% Since the reward function $r(s, a)$ is unknown to us in imitation learning, directly optimizing the above objective is intractable. We consider a more strict formulation:
% \begin{equation*}
% \min_{\pi} \sup_{r\in\mathcal{R}} \Big| \mathbb{E}_{(s, a)\sim d_{E}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a) \big] \Big|. 
% \end{equation*} 
% It is easily verified that if for every $r\in \mathcal{R}$ we have $-r \in \mathcal{R}$, then $\sup_{r\in\mathcal{R}} \big[ \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi_E}} \big[r(s, a) \big] \big]$ is non-negative, satisfies the triangular inequality, and is symmetric.
% We can then simply remove the absolute operator and formulate the problem as follows:
% \begin{equation*}
% \min_{\pi} \sup_{r\in\mathcal{R}} \Big[ \mathbb{E}_{(s, a)\sim d_{E}} \big[r(s, a)\big] - \mathbb{E}_{(s, a)\sim d_{\pi}} \big[r(s, a) \big] \Big]. 
% \end{equation*} 


% Based on Theorem~\ref{theo:policy_gradient} and actor-critic algorithm, we can actually substitute $\Psi(s_t, a_t)$ with $\hat{r}(s_t, a_t) + V(s_{t+1}) - V(s_t)$, where $V(s)$ is defined as $V(s) = \sum_{a}\pi(s, a)\Psi(s, a)$. 


% \begin{equation*}
% \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}[\phi^*(f(s, a))] =  \mathbb{E}_{(s, a)\sim d_{*}}\big[ \omega(s, a) \cdot \phi^*(f(s, a)) \big]
% \end{equation*}
% \begin{equation*}
% \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}[\nabla_\theta\log \pi_{\theta}(s, a) \cdot \Psi(s, a)] =  \mathbb{E}_{(s, a)\sim d_{*}}\big[ \omega(s, a) \cdot \nabla_\theta\log \pi_{\theta}(s, a) \cdot \Psi(s, a) \big]
% \end{equation*}
% where $\omega(s, a)$ is defined as $\omega(s, a) = \frac{d_{\pi_{\theta}}(s, a)}{d_*(s, a)}$, also called \textit{density ratio} in off-policy policy evaluation~\citep{}. 


% Problem formulation:
% \begin{align}
% \min_{\pi} \quad & D_{f} \big(d_*(s, a) || d_{\pi}(s, a) \big) \\
% \text{subject to} \quad & d_{\pi}(s, a) = (\mathcal{T} \circ d_{\pi})(s, a) \quad \forall s\in \mathcal{S}, a\in \mathcal{A}. \nonumber
% \end{align}


% Substitute the objective and plug in $x$ as $(s, a)$:
% \begin{align*}
% \min_{\pi} \sup_{\phi\in\Phi} \quad & \mathbb{E}_{(s, a)\sim d_\pi} \big[ \phi(s, a) \big] - \mathbb{E}_{(s, a)\sim d_*} \big[ f^*(\phi(s, a)) \big], \\
% \text{subject to} \quad & d_{\pi}(s, a) = (\mathcal{T} \circ d_{\pi})(s, a) \quad \forall s\in \mathcal{S}, a\in \mathcal{A}. \nonumber
% \end{align*}





% $(s, a)$ distribution
% \begin{equation}
% d(s^\prime, a^\prime) = \underbrace{(1-\gamma) d_{0}(s^\prime, a^\prime) + \gamma \int \pi(a^\prime|s^\prime) P(s^\prime|s, a) d(s, a) ds da}_{(\mathcal{T}\circ d) (s^\prime, a^\prime)}, \quad \forall (s^\prime, a^\prime)\in \mathcal{S}\times\mathcal{A}. 
% \end{equation}
