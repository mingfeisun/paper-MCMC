\section{Connections to reinforcement learning}
We now show optimizing Equation~\ref{equ:general_objective} is equivalent 
% \sw{vague: related how?} 
to actor-critic algorithms.

\subsection{Actor-Critic optimization}
Assume we parametrize $\pi$ and $f$ with $\theta$ and $\omega$ respectively. 
In order to optimize Equation~\ref{equ:general_objective}, we can iteratively solve the maximization and the minimization. 
We now show that how this $\min$-$\max$ optimization problem resembles the actor-critic algorithms in reinforcement learning. 

First, the $\sup_{f\in\mathcal{F}}$ can be directly optimized by taking the gradient of $f_{\omega}(\tau)$ and $\tilde{f}_{\omega}(\tau)$: 
\begin{equation}~\label{equ:gradient_critic}
\nabla_{\omega} \Big[ \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f_{\omega}(\tau) \big] - \mathbb{E}_{\tau\sim d_*} \big[ f_{\omega}(\tau) \big] \Big]
= \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ \nabla_{\omega} f_{\omega}(\tau) \big] - \mathbb{E}_{\tau\sim d_*} \big[ \nabla_{\omega} f_{\omega}(\tau) \big].
\end{equation}
% \vit{I am not sure what's going on because I do not know what $\tilde{f}$ is.}
Second, the $\min_{\pi}$ is wrt $\pi$, which is parameterized with $\theta$ and only included in the first term:
\begin{align*}
& \nabla_{\theta} \Big[ \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f_{\omega}(\tau) \big] - \mathbb{E}_{\tau\sim d_*} \big[ f_{\omega}(\tau) \big] \Big] 
= \nabla_{\theta} \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f_{\omega}(\tau) \big] \\
= & \nabla_{\theta} \int_{\tau} d_{\pi_{\theta}}(\tau) f_{\omega}(\tau) d\tau 
= \int_{\tau} \nabla_{\theta} d_{\pi_{\theta}}(\tau) f_{\omega}(\tau) d\tau 
= \int_{\tau}  d_{\pi_{\theta}}(\tau) f_{\omega}(\tau) \nabla_{\theta}\log d_{\pi_{\theta}}(\tau)  d\tau \\
= & \mathbb{E}_{\tau\sim d_{\pi_\theta}} \big[ f_{\omega}(\tau) \cdot \nabla_{\theta}\log d_{\pi_{\theta}}(\tau) \big].
\end{align*}
Plug in the definition of $d_{\pi_{\theta}}(\tau)$ in Equation~\ref{equ:d_pi_definition}, we have
\begin{equation*}
\nabla_{\theta} \Big[ \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f_{\omega}(\tau) \big] + \mathbb{E}_{\tau\sim d_*} \big[ \tilde{f}_{\omega}(\tau) \big] \Big] 
= \mathbb{E}_{\tau\sim d_{\pi_\theta}} \Big[ f_{\omega}(\tau) \cdot \big( \sum_{t=1}^{T}\nabla_{\theta}\log \pi_{\theta}(s_t, a_t) \big) \Big].
\end{equation*}
% Since $\pi_{\theta}$ is within samples, \sw{what does this mean?} directly computing the gradient would be intractable. However, in reinforcement learning the objective, $J(\theta)=\mathbb{E}_{s\sim d_{\pi_{\theta}}(s), a\sim\pi_{\theta}}[r(s, a)]$, shares the same form. If we interpret $f_{\omega}(s, a)$ as a reward function, we can then compute the gradient. 
% \begin{theorem}\label{theo:policy_gradient}
% For any state-action distribution $d_{\pi_{\theta}}$, in either the average or discounted cases, and any function $f_{\omega}$, 
% \begin{equation*}
% \nabla_\theta \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}\big[ f_{\omega}(s, a) \big] = \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}} \big[ \Psi_{\omega}(s, a) \cdot \nabla_\theta \log \pi_{\theta}(s, a) \big], 
% \end{equation*}
% where $\Psi_{\omega}(s, a)$ is a function defined according to $f_{\omega}(s, a)$. \sw{Defined how?  This is not precise enough for a theorem statement.}
% \end{theorem}
% \vit{You often use 'A is defined according to B'. It is not clear what it means.}

% \begin{proof}
% Note that $f(s, a)$ is a function independent of $\pi$. 
% The proof is similar to that of the policy gradient theorem in that we interpret $f(s, a)$ as the reward function, i.e., $\hat{r}(s, a) := -f(s, a)$. \sw{This is not a proof, it's not even a proof sketch.}
% 
% In the average reward formulation, $d_\pi$ is defined as $d_{\pi}=\lim_{t\rightarrow\infty}Pr(s_t=s, a_t=a|s_0, \pi)$, which we assume exists and is independent of $s_0$ for all policies. 
% The $\Psi(s, a)$ is defined as 
% \begin{equation*}
% \Psi(s, a) = \sum_{t=1}^{\infty} \mathbb{E}[f(s_t, a_t) - \rho(\pi) | s_0 = s, a_0 =a, \pi], \quad \forall s\in\mathcal{S}, a\in\mathcal{A}, 
% \end{equation*}
% where $\rho(\pi) = \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}}\big[ f(s, a) \big] $. 
% 
% 
% In the discounted reward formulation, $d_\pi$ is defined as $d_{\pi}=\sum_{t=0}^{\infty}\gamma^t Pr(s_t=s, a_t=a|s_0, \pi)$
% The $\Psi(s, a)$ is defined as 
% \begin{equation*}
% \Psi(s, a) = \mathbb{E}\big[ \sum_{k=1}^{\infty} \gamma^{k-1} f(s_{t+k}, a_{t+k}) | s_t = s, a_t =a, \pi \big], \quad \forall s\in\mathcal{S}, a\in\mathcal{A}, 
% \end{equation*}
% where $\gamma\in[0, 1)$ is a discount rate ($\gamma=1$ is allowed only in episodic tasks).
% \end{proof}


\begin{align*}
& \nabla_{\theta} \Big[ \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f(\tau) \big] - \mathbb{E}_{\tau\sim d_*} \big[ f(\tau) \big] \Big] 
= \nabla_{\theta} \mathbb{E}_{\tau\sim d_{\pi_{\theta}}} \big[ f(\tau) \big] \\
= & \nabla_{\theta} \int_{\tau} d_{\pi_{\theta}}(\tau) f(\tau) d\tau 
= \int_{\tau} \nabla_{\theta} d_{\pi_{\theta}}(\tau) f(\tau) d\tau 
= \int_{\tau}  d_{\pi_{\theta}}(\tau) f(\tau) \nabla_{\theta}\log d_{\pi_{\theta}}(\tau)  d\tau \\
= & \mathbb{E}_{\tau\sim d_{\pi_\theta}} \big[ f(\tau) \cdot \nabla_{\theta}\log d_{\pi_{\theta}}(\tau) \big].
\end{align*}

\begin{equation*}
f(\tau) = \mathbb{E}_{\tau_{*}\sim d_{*}(\tau)}[k(\tau_{*}, \tilde{\tau})] - \mathbb{E}_{\tau_{\pi}\sim d_{\pi}(\tau)}[k(\tau_{\pi}, \tilde{\tau})].
\end{equation*}

\begin{align*}
k(\tau, \tilde{\tau}) &= k\big([(s_0, a_0), (s_1, a_1), ..., (s_{T-1}, a_{T-1})], [(s_0, a_0), (s_1, a_1), ..., (s_{T-1}, a_{T-1})]\big) \\
&= k_{0}((s_0, a_0), (s_0, a_0)) \cdot k_{1}((s_1, a_1), (s_1, a_1)) \cdot ... \cdot k_{T-1}((s_{T-1}, a_{T-1}), (s_{T-1}, a_{T-1}))
\end{align*}

Combining the maximization and minimization,
we can conclude that the maximization process is to find a ``value'' for each trajectory such that the expectation over all trajectories provided in the demonstration and induced by the policy is maximized, 
while the minimization process is to find a policy that achieves optimal returns under the current estimate of the ``value''. 
This optimization process is just the actor-critic mechanism in reinforcement learning~\citep{konda2000actor}.
Specifically, in actor-critic framework, the critic uses an approximation architecture to learn a value function, which is then used to update the actor's policy parameters in a direction of performance improvement. The critic is usually updated by temporal-difference learning. In our formulation, $f_{\omega}(\tau)$ behaves exactly like the critic as it estimates a ``value'' function (for trajectories), which then provides a direction of the gradient update in the minimization process. 
The main different lies in that the critic in actor-critic algorithms is updated via temporal difference learning (which is not gradient descent of any objectives~\citep{sutton2008convergent}), 
while the critic in our formulation is updated by maximizing an explicit objective, see Equation~\ref{equ:gradient_critic}. 
To differentiate our critic from previous ones, we name $f(\tau)$ as \textit{gradient critic}, aligned with gradient temporal-difference (GTD) in~\citet{sutton2008convergent,sutton2009fast}. 

% Note that this formulation is fundamentally different from existing imitation learning methods. 
% In GAIL~\citep{ho2016generative} for example, although the actor-critic framework is adopted, e.g., trust region policy optimization~\citep{schulman2015trust}, the role of the critic is to estimate, with no bias and less variance, the ``value'' given the ``reward'' estimated by the discriminator. 
% Since the ``reward'' is always changing during the alternating process, estimating the ``value'' becomes tricky as the bias and variance are ill-posed. 
% Our formulation, however, indicates that there is no need to take a detour to first estimate the ``reward'' and then the ``value''. 
% Instead, we can directly find the critic to guide the policy update. 

% Note that the new critic can be easily converted into the classic critic where 
Define $f$ as $f(\tau)= x^{T} \cdot \phi$. 

For discounted case, $x= (1, \gamma, \gamma^{2},..., \gamma^{H-1})$, $\phi_i=r(s_i, a_i)$, where $\phi_i$ is the $i$-th element in $\phi$. 

For average case, $x= (\underbrace{1, 1,..., 1}_{H})$, and $\phi_i=r(s_i, a_i)$. 

% \sw{This is really vague: how exactly is it like AC?  Your claim is that $\Psi$ is like the critic?  But you still have to learn $f$ by optimising $\omega$, no?  That seems fundamentally different from AC where the reward is given, not learned.}


% This is also aligned with the existing distribution matching approaches in imitation learning, where the actor-critic algorithms as well as policy-gradient methods are widely used. \sw{I don't get what's novel about this observation.  In GAIL for example, you have a discriminator, which can be thought of as reward, which can be optimised using AC methods, giving three components (actor, critic, discriminator) that correspond to your $\pi$, $\Psi$, and $f$.}
% \vit{A parallel between AC methods and IRL is a super nice idea!}

% \begin{equation*}
% \min_{\pi_{\theta}} \sup_{f\in\mathcal{F}} \mathbb{E}_{\tau\sim \pi_{\theta}} \big[ f(\tau) \big] + \mathbb{E}_{\tau\sim \pi_*} \big[\tilde{f}(\tau) \big].
% \end{equation*}

% \begin{equation*}
% \nabla_{\theta} \Big[ \mathbb{E}_{\tau\sim \pi_{\theta}} \big[ f_{\omega}(\tau) \big] + \mathbb{E}_{\tau\sim \pi_*} \big[ \tilde{f}_{\omega}(\tau) \big] \Big]
% = \nabla_{\theta} \mathbb{E}_{\tau\sim \pi_{\theta}} \big[ f_{\omega}(\tau) \big]
% = \mathbb{E}_{\tau\sim \pi_{\theta}}\big[ \nabla \pi_{\theta}(\tau) f_{\omega}(\tau) \big]
% \end{equation*}
% \begin{equation*}
% \nabla_{\omega} \Big[ \mathbb{E}_{\tau\sim \pi_{\theta}} \big[ f_{\omega}(\tau) \big] + \mathbb{E}_{\tau\sim \pi_*} \big[ \tilde{f}_{\omega}(\tau) \big] \Big]
% = \mathbb{E}_{\tau\sim \pi_{\theta}} \big[ \nabla_{\omega} f_{\omega}(\tau) \big] + \mathbb{E}_{\tau\sim \pi_*} \big[ \nabla_{\omega} \tilde{f}_{\omega}(\tau) \big].
% \end{equation*}


\subsection{Deterministic adversarial imitation}
One of the direct outcomes of the above connection to policy gradient is a novel algorithm, which we call Deterministic Adversarial Imitation Learning. 
Assume policy $\pi_{\theta}$ is deterministic, then based on the deterministic policy gradient, we have
% \begin{align*}
% & \nabla_{\theta} \Big[ \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}} \big[ f_{\omega}(s, a) \big] + \mathbb{E}_{(s, a)\sim d_*} \big[ \tilde{f}_{\omega}(s, a) \big] \Big] 
% = \nabla_{\theta} \mathbb{E}_{(s, a)\sim d_{\pi_{\theta}}} \big[ f_{\omega}(s, a) \big] \\ 
% = & \mathbb{E}_{s\sim d_{\pi_{\theta}}(s)} \big[  \nabla_\theta \pi_{\theta}(s, a) \cdot \nabla_a \Psi(s, a)\big|_{a=\pi_{\theta}(s)}\big], 
% \end{align*}

The algorithm runs as follows:

\begin{algorithm}
\caption{Deterministic Adversarial Imitation Learning}
\begin{algorithmic}[1] %enables line numbers
\STATE Initialize critic network $Q_{\omega_1}$, $Q_{\omega_2}$, actor network $\pi_{\theta}$ and reward network $r_{\phi}$ wth random parameters $\omega_1$, $\omega_2$, $\theta$ and $\phi$
\STATE Initialize target networks $\omega_1^\prime\rightarrow\omega_1$, $\omega_2^\prime\rightarrow\omega_2$, $\theta^\prime\rightarrow\theta$,
\STATE Initialize replay buffer $\mathcal{B}$
\STATE Initialize demonstration set $\mathcal{D}$
\FOR{$t=1$ to $T$}
\STATE Select action with exploration noise $a\sim\pi_{\theta} + \epsilon$, $\epsilon\sim \mathcal{N}(0, \sigma)$ and observe new state $s^\prime$
\STATE Store transition tuple $(s, a, s^\prime)$ in $\mathcal{B}$
\STATE Sample mini-batch of $N$ transitions $(s, a, s^\prime)$ from $\mathcal{B}$
\IF{$t$ mod $d_r$}
\STATE Sample mini-batch of $N$ transitions $(s, a, s^\prime)$ from $\mathcal{D}$
\STATE Optimize $r_{\phi}$ based on the objective
\ENDIF
\STATE $\tilde{a} \leftarrow \pi_{\theta^\prime}+\epsilon$, $\epsilon\sim \text{clip}(\mathcal{N}(0, \tilde{\sigma}, -c, c))$
\STATE $y\rightarrow r_{\phi}+\gamma \min_{i=1, 2}Q_{\phi_i}^\prime(s^\prime, \tilde{a})$
\STATE Update critics $\omega_i\leftarrow \arg\min_{\omega_i} N^{-1}\sum(y - Q_{\omega_i}(s, a))^2$
\IF{$t$ mod $d_\pi$}
\STATE Update $\theta$ by the deterministic policy gradient
\STATE Update target networks: $\omega_i^\prime\leftarrow \tau \omega_i + (1-\tau)\omega_i^\prime$, $\theta^\prime \leftarrow \tau \theta + (1-\tau)\theta $
\ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}
