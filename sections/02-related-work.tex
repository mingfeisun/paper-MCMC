\section{Related work}
Distribution matching approaches 

In recent years, the development of Adversarial Imitation Learning has been mostly focused on on-policy algorithms. 
After~\citet{ho2016generative} proposed GAIL to perform imitation learning via adversarial training, a number of extensions has been introduced. 
Many of these applications of the AIL framework~\citep{li2017infogail,hausman2017multi,sun2019adversarial} maintain the same form of distribution estimation as GAIL which necessitates on-policy samples. 


The connections between RL and divergence minimization have long been studied in the rich prior literature of control as probabilistic inference~\citep{todorov2007linearly,toussaint2009robot,peters2010relative,kappen2012optimal}. 
Specifically, they have shown that optimal control under entropy regularization can be viewed as approximate inference on a graphical model, or equivalently minimizing reverse KL divergence between reward-weighted trajectory and policy trajectory distributions~\citep{kappen2012optimal,levine2018reinforcement}.
Building on such intuitions, a number of work extended RL algorithms based on picking another divergence metric, such as forward KL~\citep{peters2007reinforcement,norouzi2016reward}, and demonstrated substantially improved empirical performances in certain situations. 
Our work draws significant inspirations from these prior works in RL and aims to provide a probabilistic perspective in Imitation Learning (IL).

In the field of robotics, imitation learning (IL), or bootstrapping from IL, has often been the method of choice over RL due to difficulty in exploration and scarcity of data~\citep{} [3, 4, 5, 6]. 
While Behavioural Cloning (BC) is the most widely used IL algorithm due to the simplicity of its objective, it suffers from the problem of covariate shift between train and test time. 
Methods such as DAgger [27] and Dart [28] aim to relieve this mismatch, yet assume interactive access to expert policies.


Inverse Reinforcement Learning (IRL) algorithms have shown promising results in challenging continuous control problems [29, 30, 2, 1, 7], outperforming BC. 
Similarly to RL, the connections between IRL and divergence minimization have long been alluded. 
Early works in IRL operated by matching feature expectations or moments [9] between policies and experts, a popular approach in distribution matching [31, 32]. 
Furthermore, Maximum Entropy (Max-Ent) IRL [21, 22] — an IRL framework that addresses degeneracies of the original IRL formulation [10, 11] — is explicitly formulated as an energy-based modeling problem. 
Recent scalable approaches to Max-Ent IRL [2, 1, 12], motivated by adversarial approaches to generative modeling [33], demonstrate additional connections to distribution matching. 
Our work generalizes the objective proposed in [1, 12] based on recent insights from generative modeling [15], and further provides a unified perspective for viewing common IL algorithms. 
Concurrent to our work, Ke et al. [34] also present a unifying probabilistic perspective on IL; 
however, their empirical experiments solely focus on grid world domains, while our work provides comparative results on high-dimensional continuous control environments and also evaluates the effectiveness of IL algorithms for state marginal matching [16].
